{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gibbs sampling for linear regression in Python\n",
    "\n",
    "Kieran Campbell - kieranc [at] well.ox.ac.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian linear regression\n",
    "\n",
    "Here we are interested in Gibbs sampling for normal linear regression with one independent variable. We assume we have paired data  \\\\( (y_i, x_i) , i = 1, \\ldots, N \\\\). We wish to find the posterior distributions of the coefficients \\\\(\\beta_0\\\\) (the intercept), \\\\(\\beta_1\\\\) (the gradient) and of the precision $\\tau$ which is the reciprocal of the variance. The model can be written as \n",
    "\n",
    "$$ y_i \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_i, 1 / \\tau) $$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$$ y_i = \\beta_0 + \\beta_1 x_i + \\epsilon, \\; \\; \\epsilon \\sim \\mathcal{N}(0, 1 / \\tau) $$\n",
    "\n",
    "The likelihood for this model may be written as\n",
    "\n",
    "$$ L(y_1, \\ldots, y_N, x_1, \\ldots, x_N | \\beta_0, \\beta_1, \\tau) = \\prod_{i = 1}^N \\mathcal{N}(\\beta_0 + \\beta_1 x_i, 1 / \\tau) $$\n",
    "\n",
    "We also wish to place [conjugate priors](https://en.wikipedia.org/wiki/Conjugate_prior) on \\\\(\\beta_0\\\\), \\\\(\\beta_1\\\\) and \\\\(\\tau\\\\) for reasons that will become apparent later. For these we choose\n",
    "\n",
    "$$ \\beta_0 \\sim \\mathcal{N}(\\mu_0, 1 / \\tau_0) $$\n",
    "\n",
    "$$ \\beta_1 \\sim \\mathcal{N}(\\mu, 1 / \\tau_1) $$\n",
    "\n",
    "$$ \\tau \\sim \\text{Gamma}(\\alpha, \\beta) $$\n",
    "\n",
    "\n",
    "\n",
    "### Gibbs sampling\n",
    "\n",
    "[Gibbs sampling](https://en.wikipedia.org/wiki/Gibbs_sampling) works as follows: suppose we have two parameters \\\\(\\theta_1\\\\) and \\\\(\\theta_2\\\\) and some data \\\\(x\\\\). Our goal is to find the posterior distribution of \\\\(p(\\theta_1, \\theta_2 \\| x)\\\\). To do this in a Gibbs sampling regime we need to work out the conditional distributions \\\\(p(\\theta_1 \\| \\theta_2, x)\\\\) and \\\\(p(\\theta_2 \\| \\theta_1, x)\\\\) (which is typically the hard part). The Gibbs updates are then\n",
    "\n",
    "1. Pick some initial \\\\(\\theta_2^{(i)}\\\\).\n",
    "2. Sample \\\\( \\theta_1^{(i+1)} \\sim p(\\theta_1 \\| \\theta_2^{(i)}, x) \\\\)\n",
    "3. Sample \\\\( \\theta_2^{(i+1)} \\sim p(\\theta_2 \\| \\theta_1^{(i+1)}, x) \\\\)\n",
    "\n",
    "Then increment \\\\(i\\\\) and repeat \\\\(K\\\\) times to draw \\\\(K\\\\) samples. This is equivalent to sampling new values for a given variable *while holding all others constant*. The key thing to remember in Gibbs sampling is to always use the most recent parameter values for all samples (e.g. sample \\\\(\\theta_2^{(i+1)} \\sim p(\\theta_2 \\| \\theta_1^{(i+1)}, x)\\\\) and not \\\\(\\theta_2^{(i+1)} \\sim p(\\theta_2 \\| \\theta_1^{(i)}, x)\\\\) provided \\\\(\\theta_1^{(i+1)}\\\\) has already been sampled).\n",
    "\n",
    "The massive advantage of Gibbs sampling over other MCMC methods (namely Metropolis-Hastings) is that no tuning parameters are required! The downside is the need of a fair bit of maths to derive the updates, which even then aren't always guaranteed to exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pythonic setup\n",
    "\n",
    "First let's set ourselves up with python imports and functions so we can implement the functions as we derive them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving a Gibbs sampler\n",
    "\n",
    "The general approach to deriving an update for a variable is \n",
    "\n",
    "1. Write down the posterior conditional density in log-form\n",
    "2. Throw away all terms that don't depend on the current sampling variable\n",
    "3. Pretend this is the density for your variable of interest and all other variables are fixed. What distribution does the log-density remind you of?\n",
    "4. That's your conditional sampling density!\n",
    "\n",
    "We go through this for our three variables step by step below.\n",
    "\n",
    "#### Updates for \\\\(\\beta_0\\\\)\n",
    "\n",
    "We're interested in finding\n",
    "\n",
    "$$ p(\\beta_0 | \\beta_1, \\tau, y, x) \\propto p(y, x | \\beta_0, \\beta_1, \\tau) p(\\beta_0) $$\n",
    "\n",
    "\n",
    "Note that \\\\(p(y, x \\| \\beta_0, \\beta_1, \\tau)\\\\) is just the likelihood from above and \\\\(p(\\beta_0)\\\\) is simply \\\\(\\mathcal{N}(\\mu_0, 1 / \\tau_0)\\\\). \n",
    "\n",
    "If a variable \\\\(x\\\\) follows a normal distribution with mean \\\\(\\mu\\\\) and precision \\\\(\\tau\\\\) then the log-dependence on \\\\(x\\\\) is \\\\(-\\frac{\\tau}{2}(x - \\mu)^2 \\propto -\\frac{\\tau}{2} x^2 + \\tau \\mu x\\\\). So if we can force the log-posterior conditional density into a quadratic form then the coefficient of \\\\(x^2\\\\) (where \\\\(x\\\\) is the variable of interest) will be \\\\(\\tau \\mu\\\\) and the coefficient of \\\\(x^2\\\\) will be \\\\(-\\frac{\\tau}{2}\\\\). \n",
    "\n",
    "Hence the log-dependence on \\\\(\\beta_0\\\\) is\n",
    "\n",
    "$$ -\\frac{\\tau_0}{2}(\\beta_0 - \\mu_0)^2 - \\frac{\\tau}{2} \\sum_{i=1}^N (y_i - \\beta_0 - \\beta_1 x_i)^2 $$\n",
    "\n",
    "Although it's perhaps not obvious, this expression is quadratic in \\\\(\\beta_0\\\\), meaning the conditional sampling density for \\\\(\\beta_0\\\\) will also be normal. A bit of algebra (dropping all terms that don't involve \\\\(\\beta_0\\\\) takes us to\n",
    "\n",
    "$$ -\\frac{\\tau_0}{2} \\beta_0^2 +\\tau_0 \\mu_0 \\beta_0 -\\frac{\\tau}{2} N \\beta_0^2 \n",
    "+ \\tau \\sum_i (y_i - \\beta_1 x_i) \\beta_0$$\n",
    "\n",
    "In other words the coefficient of \\\\(\\beta_0\\\\) is \\\\(\\tau_0 \\mu_0 + \\tau \\sum_i (y_i - \\beta_1 x_i) \\\\) while the coefficient of \\\\(\\beta_0^2\\\\) is \\\\(-\\frac{\\tau_0}{2} -\\frac{\\tau}{2} N\\\\). This implies the conditional sampling distribution of \\\\(\\beta_0\\\\) is\n",
    "\n",
    "$$ \\beta_0 | \\beta_1, \\tau, \\tau_0, \\mu_0, x, y \\sim \\mathcal{N}\\left( \\frac{\\tau_0 \\mu_0 + \\tau \\sum_i (y_i - \\beta_1 x_i)}{\\tau_0 + \\tau N}, 1 / (\\tau_0 + \\tau N) \\right) $$\n",
    "\n",
    "Let's turn that into a python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_beta_0(y, x, beta_1, tau, mu_0, tau_0):\n",
    "    N = len(y)\n",
    "    assert len(x) == N\n",
    "    precision = tau_0 + tau * N\n",
    "    mean = tau_0 * mu_0 + tau * np.sum(y - beta_1 * x)\n",
    "    mean /= precision\n",
    "    return np.random.normal(mean, 1 / np.sqrt(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet! Now back to the maths.\n",
    "\n",
    "#### Update for \\\\(\\beta_1\\\\)\n",
    "\n",
    "Similarly to \\\\(\\beta_0\\\\), the dependence of the conditional log-posterior is given by\n",
    "\n",
    "$$ -\\frac{\\tau_1}{2}(\\beta_1 - \\mu_1)^2 - \\frac{\\tau}{2} \\sum_{i=1}^N (y_i - \\beta_0 - \\beta_1 x_i)^2 $$\n",
    "\n",
    "which if we expand out and drop all terms that don't include \\\\(\\beta_1\\\\) we get\n",
    "\n",
    "$$ -\\frac{\\tau_1}{2} \\beta_1^2 +\\tau_1 \\mu_1 \\beta_1 -\\frac{\\tau}{2} \\sum_i x_i^2 \\beta_1^2 \n",
    "+ \\tau \\sum_i (y_i - \\beta_0) x_i \\beta_1$$\n",
    "\n",
    "so the coefficient of \\\\(\\beta_1\\\\) is \\\\(\\tau_1 \\mu_1 + \\tau  \\sum_i (y_i - \\beta_0) x_i\\\\) while the coefficient of \\\\(\\beta_1^2\\\\) is  \\\\(-\\frac{\\tau_1}{2} -\\frac{\\tau}{2} \\sum_i x_i^2\\\\). Therefore the conditional sampling density of \\\\(\\beta_1\\\\) is \n",
    "\n",
    "$$ \\beta_1 | \\beta_0, \\tau, \\mu_1, \\tau_1, x, y \\sim \\mathcal{N}\\left( \\frac{\\tau_1 \\mu_1 + \\tau  \\sum_i (y_i - \\beta_0) x_i}{\\tau_1 + \\tau \\sum_i x_i^2}, 1 / (\\tau_1 + \\tau \\sum_i x_i^2) \\right) $$\n",
    "\n",
    "Let's turn that into a Python function too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_beta_1(y, x, beta_0, tau, mu_1, tau_1):\n",
    "    N = len(y)\n",
    "    assert len(x) == N\n",
    "    precision = tau_1 + tau * np.sum(x * x)\n",
    "    mean = tau_1 * mu_1 + tau * np.sum( (y - beta_0) * x)\n",
    "    mean /= precision\n",
    "    return np.random.normal(mean, 1 / np.sqrt(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update for \\\\(\\tau\\\\)\n",
    "\n",
    "Deriving the Gibbs update for \\\\(\\tau\\\\) is the trickiest part of this exercise as we have to deal with non-Gaussian distributions. First let's introduce the Gamma distribution, parametrised by \\\\(\\alpha\\\\) and \\\\(\\beta\\\\). Up to the normalising constant the probability of an observation \\\\(x\\\\) under a Gamma density is given by\n",
    "$$ p(x; \\alpha, \\beta) \\propto \\beta^\\alpha x^{\\alpha - 1} e^{-\\beta x} $$\n",
    "and so the log-dependency of any terms involving \\\\(x\\\\) is given by\n",
    "$$ l(x; \\alpha, \\beta) \\propto (\\alpha - 1) \\log x - \\beta x $$\n",
    "\n",
    "Now back to our derivation. We want\n",
    "\n",
    "$$ p(\\tau | \\beta_0, \\beta_1, y, x) \\propto p(y, x | \\beta_0 \\beta_1, \\tau) p(\\tau) $$\n",
    "\n",
    "which in this case is a density of \n",
    "\n",
    "$$ \\prod_{i = 1}^N \\mathcal{N}(\\beta_0 + \\beta_1 x_i, 1 / \\tau) \\times \\text{Gamma}(\\tau | \\alpha, \\beta) $$\n",
    "\n",
    "The key question to ask here is, *what's the density of \\\\(\\tau\\\\) assuming all other parameters are held constant*? If we look at the log density of this expression we get \n",
    "\n",
    "$$ \\frac{N}{2} \\log \\tau - \\frac{\\tau}{2} \\sum_i (y_i - \\beta_0 - \\beta_1 x_i)^2 + (\\alpha - 1) \\log \\tau - \\beta \\tau $$\n",
    "\n",
    "which has a coefficient of \\\\(\\tau\\\\) of \\\\( - \\sum_i \\frac{(y_i - \\beta_0 - \\beta_1 x_i)^2}{2} - \\beta\\\\) and a coefficient of \\\\(\\log \\tau\\\\) of \\\\(\\frac{N}{2} + \\alpha - 1\\\\). If you look at the equation of the log-density of the Gamma distribution above, this implies that \\\\(\\tau\\\\) as a conditional sampling density of\n",
    "\n",
    "$$ \\tau | \\beta_0, \\beta_1, \\alpha, \\beta, x, y \\sim \\text{Gamma} \\left( \\alpha + \\frac{N}{2}, \\beta + \\sum_i \\frac{(y_i - \\beta_0 - \\beta_1 x_i)^2}{2} \\right) $$\n",
    "\n",
    "We can now code this into python. `np.random.gamma` uses the shape and scale parameterisation of a Gamma distribution, where the shape \\\\(k = \\alpha\\\\) but the scale \\\\(\\theta = 1 / \\beta\\\\), so we need to invert our expression for \\\\(\\beta\\\\) before sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_tau(y, x, beta_0, beta_1, alpha, beta):\n",
    "    N = len(y)\n",
    "    alpha_new = alpha + N / 2\n",
    "    resid = y - beta_0 - beta_1 * x\n",
    "    beta_new = beta + np.sum(resid * resid) / 2\n",
    "    return np.random.gamma(alpha_new, 1 / beta_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some synthetic data\n",
    "\n",
    "To test our Gibbs sampler we'll need some synthetic data. Let's keep things simple - set \\\\(\\beta_0 = -1\\\\), \\\\(\\beta_1 = 2\\\\) and \\\\( \\tau = 1 \\\\):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x108488400>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAECCAYAAADjBlzIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEsZJREFUeJzt3W+MXFd5x/HvutnELd1ABQsUNYi0ap8WqQkSUZKNgg0F\njCKnrleUekWTMKahhEbhT1sMIYI3/CmhKBARiFUgXhJoHZKybo1LcCCRUyI3UalQg0pPQLit1DaK\nQQ3Zkg528PTFzCbe9a5n5vrOzD13vp83ntnc2XuO1/nds885956JVquFJClf60bdAEnSqTHIJSlz\nBrkkZc4gl6TMGeSSlDmDXJIyd1qvB0bEBcCHU0qviIhfAeaBY8C3U0pXD6h9kqQuehqRR8Q7gU8D\nZ3S+dAPwnpTSRmBdRPzOgNonSeqi19LK94DZ496/NKX0953XXwFeVWqrJEk96ynIU0oLwJPHfWni\nuNeLwDPLbJQkqXdFJzuPHfd6CnishLZIkgroebJzhX+KiA0ppfuAS4B7un2g1Wq1JiYmuh0mSVqu\na3AWDfI/BT4dEZPAd4A7u7ZkYoLDhxcLnq76pqenatu/OvcN7F/uxqF/3fQc5Cmlfwcu6rz+LvDy\nog2TJJXHG4IkKXMGuSRlziCXpMwZ5JKUOYNckjJnkEtS5gxyScqcQS5JmTPIJSlzBrkkZc4gl6TM\nGeSSlDmDXJIyZ5BLUuYMcknKnEEuSZkzyCUpcwa5JGXOIJekzBnkkpQ5g1ySMmeQS1LmDHJJypxB\nLkmZM8glKXMGuSRlziCXpMydNuoGSNKSZrPJ7t33ATA3t4H169ePuEV5MMglVUKz2WTbtgUOHtwO\nwMLCLm6/fdYw70GhII+ICeAzQAA/Bd6UUnq4zIZJGi+7d9/XCfFJAA4ebLB7914ajU2jbVgGitbI\nNwHPSCldDLwf+FB5TZIk9aNokDeBZ3ZG5s8EjpTXJEnjaG5uAzMzu2jHyRFmZuaZm9sw6mZloWiN\n/BvAzwL/CjwbuLS0FkkaS+vXr+f222fZvXsvAHNz1sd7VXREvgO4P6UUwLnArRFxennNkjSO1q9f\nT6OxiUZjkyHeh6Ij8p8HftR5/Vjn+/xMtw9NT08VPF0e6ty/OvcN7F/u6t6/biZarVbfH4qIZwG7\ngOfQDvGPp5Ru7/Kx1uHDi/23MBPT01PUtX917hvYv9yNQf8muh1TaESeUnoMmC3yWUlSubxFX5Iy\nZ5BLUuYMcknKnEEuSZkzyCUpcwa5JGXOIJekzBnkkpQ5g1ySMmeQS1LmDHJJypx7dkpjqteNjt0Q\nufoMcmkM9brRsRsi58HSijSGlm90PNnZ6Pi+wsdptAxyScqcQS6NoV43OnZD5DxYI5fGUK8bHbsh\nch4McmlMLW10XNZxGh1LK5KUOYNckjJnkEtS5gxyScqcQS5JmTPIJSlzBrkkZc4gl6TMGeSSlDmD\nXJIyZ5BLUuYMcknKXOGHZkXEu4Etne9xU0rp1tJaJUnqWaEReURsBGZSShcBrwB+udRWSRp7zWaT\n+fn9zM/vp9lsjro5lVZ0RP4a4NsRsQeYAt5ZXpOk8eCmxmtzr9D+FA3y5wAvBC6lPRr/W+DXy2qU\nVFVlha9BdXLL9wqls1foXp+LvoaiQf5D4DsppSeBhyOiGRHPSSn94GQfmp6eKni6PNS5f3XuG/TW\nv2azyetedwcHDlwOwL59t3HXXa8vFL47d54YVPv23c1VV23u+3v1Iref39TUiX+nU1Pr1+xHbv0r\nW9Eg/wbwVuBjEfEC4Odoh/tJHT68WPB01Tc9PVXb/tW5b9B7/+bn93dCvB2+Bw5cxic+UWyUuLh4\nYs13cbE5kL/nHH9+mzefz8zMLg4ebAAwMzPP5s2zq/Yjx/71o5eLVKEgTynti4iXRcSDwATwRyml\nVpHvJY2jubkNLCwsD6q5udnRNqokvZafTnace4X2Z6LVGlr+tup+1axr/+rcN+i9f0/XtRtAO3xP\npa49rMnOYf78Vtb+Z2ZWr/33elwvxuDf50S3Y9x8WepR2aPEQW1qvPIC0V5YNhy9TlI6mVkug1zq\nw6h3lO82il9tNcw991wx9HZquLxFX8rEUkjv2LGFHTu2sG3bwgk3yiwf6U5y8GCD+fmvD62Nc3Mb\nmJnZBRwBjnRq/xsKH6feOCKXMrFaOeK2277E5GT7fRWCsNfyk5OZ5TLIpYx95jPf4tCh9wPtMsrn\nPnfJCathGo0rWFw8OrQ29Vp+GnWZqk4McikTK5csnn32xzh06DqOH6Hv2bN31ZHuMINcw2eQSxm5\n9NIpnv/86znvvF8FXsh111mOkEEuZWHlapRHHmmXUb785eVllK1bL3HVyhgyyKUMrDbRuVoZZbXj\n5ufv5rWvHf1EqAbHIJcy5oShwHXkUhZOZX12o/HKIbdWw+aIXMrAqa7PdtVKvRnkUiZcn621GOSS\nTuA2dHkxyCUt4zZ0+XGyU9Iyqz14a2l0rmpyRC4NiOUJDYsjcmkAennkbFX5iNn8OCKXBiDnHXB6\nWerobxvVYpBLOsHJljA6GVo9llakAci5PNFsNpmf38/8/P5Vy0FOhlaPI3JpAHLdAcfRdp4ckUsD\nslSeaDQ2VTIIVxt59zLazvm3jbpyRC6NobVG3r3I9beNOjPIpTG01qqaldvJtUfbJwa8z3OpFoNc\n0lP6GW27BLE6DHIpQ6caoicbefcy2nZStFoMcikzZYRoryPv4y8YW7eez549DwJw9OiRbG94qiOD\nXMpMWXeNdht5L79gNPnAB27k8cffBcDZZ98ANJ9qg0brlJYfRsRzI+I/IuLXymqQpGpYfsG4rxPi\n7WWJhw69g7PP/iAuQayGwiPyiDgN2Ak8UV5zpPobZH17mK688iVMTroEsQpOpbTyUeBm4NqS2iIN\nxcoghamhnntY9e1TtfyC8TLOPPN6Hn98B9C+eFx+ueFdFYWCPCIawKMppbsj4j3lNkkanNWC9J57\nrhja+YdV3y7DygvG1q1vYM8eR+BVNNFqtfr+UEQcAI513r4ESMCWlNKjJ/lY/yeSSrZz5z7e8pZN\nPD1Jd4Sbb76bq67aPBbnV5Ymuh1QaESeUtq49Doi7gXe3CXEATh8eLHI6bIwPT1V2/7VqW+Li6tv\n7jCs/m3efD4zM8vr25s3zw70/HX6+a1mHPrXTRnLDx1pKxurTRQ2GlewuHh0KOf3OSUahEKllYJa\ndb9q1rV/devbysnOs86arkz/BnHbe91+fiuNQf8GU1qRclbVBz5527uK8nnkUkW4846KMsglKXMG\nuVQR7ryjoqyRSxXhihYVZZCr1nLb/KCqE7GqNoNctVWVVSC5XUyUH2vkqq0qrAJZupjs2LGFHTu2\nsG3bwlM71ktlMcilAarCxUT1Z5CrtlwFonFhjVy1VYVVIFXZBEL1ZpCrNFWc1Bv1KpAqXExUfwa5\nSlGVFSJVNOqLierPGrlK4aSeNDoGuSRlziBXKVwhIo2ONXKVYpCTelWcRJWqxCBXaQYxqVdkEtXg\n17ixtKJK63cS1VviNY4MctXKicE/x9vetpP5+f0GumrLIFclNZtN5uf3c/ToES688LMUm0RtAl9k\nYeHdjs5VaxOtVmtY52rVfKfr2u7kPey+rayLX3DBX7Bly7OZnJzsWvN++rMN4O+AzbRH5wBHmJ29\nnpmZFy/7PnX+2YH9y9309NREt2MckatyVpZHHnjgTUxOTtJobOo6cbm0euYjH9nL7OxDJ/z3hYVz\nShudL/3WYNlGo2aQq3aWVs/ceONVy9a2w63AayjjzlMnVVUlBrkqp6ybi5aPzq8HtgHlLEX0kQSq\nEteRq3LKvLloaXQ+N7eBRx75Kx8nq1pysrMkdZ5wqUvf1rpRqEj/lk+qti8MVX3aY11+fmsZg/51\nnex0RK6xUeadpz5nXFVikCtLVbgN3+eMqyoKBXlEnAbcArwIOB34YEppb4ntktbkJhbSckVXrVwG\n/CCltAG4BLipvCZJJ+eKEWm5oqWVLwJ3dF6vA46W0xxJUr8KjchTSk+klH4cEVO0A/26cpslrc1N\nLKTlCi8/jIizgC8BN6WUPtfDR4a2zlH11749/usANBqvtD6uOuu6/LBQkEfE84B7gatTSvf2+DHX\nkWeqzn0D+5e7MejfwNaRXws8C3hvRLyP9mj7kpTSTwp+P2WmCsv/JLUVCvKU0tuBt5fcFmXC5X9S\ntfjQLPXN5X9StXhn55BZkpBUNkfkQ1SXZ1i7/E+qFkfkQ7S8JEGnJLE3u+d1+MAoqVoMchXiA6Ok\n6rC0MkSDLkm4h6Q0nhyRD9EgSxIuCZTGl0E+ZIMqSdSl/i6pfwa5hsall9JgWCOviaovCazL0kup\nihyR10TVlwT2W/px9C71ziCvkbosCXTiVuqPpRUNxWqln61bz191uaTPcpH644hcQ7Gy9LN16yW8\n4Q1fcdQtlcARuYZmqfTTaGxiz54H1xx1V33iVqoaR+SqnKpP3EpVY5BrJObmNrCwsIuDBxsAnVH3\n7FP/vS4Tt9IwGOQD4vK5k3PULZXHIB8Al8/1xlG3VA4nOwfA5XOShskgl6TMGeQDUMflcz7rXKou\na+QDULeJPGv+UrUZ5ANSp4m8+fmv+6xzqcIsrUhS5gxyddVovLJ2NX+pTiytqKu61fylujHIx1g/\nd5/WqeYv1Y1BPqZciSLVR6Egj4gJ4FPAuUATuDKl9P0yG6bB6nfrNUnVVXSycytwRkrpIuBa4Iby\nmjTevPFGUr+KBvnFwF0AKaUHgPNKa9EYK3On+W4XhDrefSqNq6I18jOBHx33/smIWJdSOlZCm8ZW\nWeWOXurfrkSR6qNokD8OTB333hCvkF4vCK5EkeqhaJDfD1wK3BkRFwIP9fKh6emp7gdl7FT7d801\nm9m37zYOHLgMgI0bP88117y+75Hy1NSJx09NrT+l9vmzy5v9q7eJVqvV94eOW7VyTudL21NKD3f5\nWOvw4cW+z5WL6ekpyuhfGTsLPV1aaQDtbdROZWlhWX2rKvuXtzHo30S3YwoFeUEG+RCVudVc1fpW\nNvuXtzHoX9cg94agmrL+LY0Pg7wi3KxZUlEGeQVU6XZ5LyhSfnyMbQVUZbPmMm9IkjQ8BrmeUpUL\niqT+GOQV4O3ykk6Fyw9LcqpLoJZq00ePHgVaTE6ePvQa9Vrrz886a7ruy7vsX8bGoH8uP8zF+vXr\nmZvbMNJJT5+/IuXJIK+QKjwj3PXnUn6skUtS5gzyCnHSU1IRllYqxBq1pCIM8oqxRi2pXwa5AG/N\nl3JmkKtSz3qR1D8nO+Wt+VLmDHJJypxBLpc9SpmzRi6XPUqZM8gFuOxRypmlFUnKnEEuSZkzyCUp\ncwa5JGXOIJekzBnkkpQ5g1ySMmeQS1LmDHJJylyhOzsj4kzg88CZtB+Z9ycppX8os2GSpN4UHZH/\nMfC1lNLLge3AJ0trkSSpL0WftXID8JPO60ng/8ppjiSpX12DPCLeCLwDaAETnT+3p5S+GRHPB24D\n3jrQVkqS1tQ1yFNKtwC3rPx6RPwm8Je06+PfGEDbJEk9mGi1Wn1/KCJeDPw18HsppYdKb5UkqWdF\ng3wPcA7wb7TLLY+llGbLbZokqReFglySVB3eECRJmTPIJSlzBrkkZc4gl6TMFb2zs5CImAV+N6X0\n+8M876BExATwKeBcoAlcmVL6/mhbVb6IuAD4cErpFaNuS5ki4jTa90i8CDgd+GBKae9IG1WiiFgH\nfBoI4BhwVUrpX0bbqnJFxHOBfwRelVJ6eNTtKVNEfBP4UeftoZTSH6x17NCCPCI+DmwCvjWscw7B\nVuCMlNJFnbC7ofO12oiIdwKXA/876rYMwGXAD1JKV0TEL9D+t1mbIAd+G2illC6OiI3Ah6jRv8/O\nhXgn8MSo21K2iDgDIKX0W70cP8zSyv3AW4Z4vmG4GLgLIKX0AHDeaJszEN8D6nqPwBeB93ZerwOO\njrAtpUsp/Q3wh523LwL+Z3StGYiPAjcD/zXqhgzAucAzIuKrEfG1zkBxTaUHeUS8MSIeioh/Pu7P\nl6aU7ij7XBVwJk//6gPwZOfX2dpIKS0AT466HYOQUnoipfTjiJgC7gCuG3WbypZSOhYRu4AbgS+M\nuj1liYgG8GhK6W7aNyXWzRPAn6eUXkN7APyFk2VL6aWVtZ7NUlOPA1PHvV+XUjo2qsaofxFxFvAl\n4KaU0u2jbs8gpJS2R8S7gAcj4jdSSnV4Wul24FhEvBp4CXBrRGxJKT064naV5WHavw2TUvpuRPwQ\n+EXgP1c7eKiTnTV0P3ApcGdEXAjU+bkztRv1RMTzgK8CV6eU7h11e8oWEZcDv5RS+jPak/E/pT3p\nmb2U0sal1xFxL/DmGoU4tC9U5wBXR8QLaA8Y/3utgw3yU7MAvDoi7u+83z7KxgxYHZ/lcC3wLOC9\nEfE+2n28JKX0k5N/LBt3AvMRcYD2/+tvq1HfjlfHf5ufBW6JiPto9++NJ/tt32etSFLmajUxJ0nj\nyCCXpMwZ5JKUOYNckjJnkEtS5gxyScqcQS5JmTPIJSlz/w+5URD/LkkzPQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108bae5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "beta_0_true = -1\n",
    "beta_1_true = 2\n",
    "tau_true = 1\n",
    "\n",
    "N = 50\n",
    "x = np.random.uniform(low = 0, high = 4, size = N)\n",
    "y = np.random.normal(beta_0_true + beta_1_true * x, 1 / np.sqrt(tau_true))\n",
    "\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing our Gibbs sampler\n",
    "\n",
    "Now we're ready to write the Gibbs sampler. Apart from the data we need to supply initial parameter estimates and hyper parameters. Note that technically we only need to supply all but one of the initial parameters and one of them will be sampled first and that value used for the rest of that iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## specify initial values\n",
    "init = {\"beta_0\": 0,\n",
    "        \"beta_1\": 0,\n",
    "        \"tau\": 2}\n",
    "\n",
    "## specify hyper parameters\n",
    "hypers = {\"mu_0\": 0,\n",
    "         \"tau_0\": 1,\n",
    "         \"mu_1\": 0,\n",
    "         \"tau_1\": 1,\n",
    "         \"alpha\": 2,\n",
    "         \"beta\": 1}\n",
    "\n",
    "def gibbs(y, x, iters, init, hypers):\n",
    "    assert len(y) == len(x)\n",
    "    beta_0 = init[\"beta_0\"]\n",
    "    beta_1 = init[\"beta_1\"]\n",
    "    tau = init[\"tau\"]\n",
    "    \n",
    "    trace = np.zeros((iters, 3)) ## trace to store values of beta_0, beta_1, tau\n",
    "    \n",
    "    for it in range(iters):\n",
    "        beta_0 = sample_beta_0(y, x, beta_1, tau, hypers[\"mu_0\"], hypers[\"tau_0\"])\n",
    "        beta_1 = sample_beta_1(y, x, beta_0, tau, hypers[\"mu_1\"], hypers[\"tau_1\"])\n",
    "        tau = sample_tau(y, x, beta_0, beta_1, hypers[\"alpha\"], hypers[\"beta\"])\n",
    "        trace[it,:] = np.array((beta_0, beta_1, tau))\n",
    "        \n",
    "    return trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.11402287,  0.41004987,  0.26934712],\n",
       "       [ 2.23863274,  0.72516157,  0.52218892],\n",
       "       [ 1.87998455,  0.99068647,  0.53321582],\n",
       "       ..., \n",
       "       [-0.38212999,  1.76538648,  0.64028601],\n",
       "       [-0.31500311,  1.64138505,  0.90244681],\n",
       "       [-0.0808753 ,  1.66417406,  0.73124862]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iters = 1000\n",
    "trace = gibbs(y, x, iters, init, hypers)\n",
    "trace"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
